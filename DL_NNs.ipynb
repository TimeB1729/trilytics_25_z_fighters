{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "290f8180-b81f-47aa-836b-e2fb85649577",
   "metadata": {},
   "source": [
    "# Initializing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a63e58c4-7b5b-462e-a08c-859b1efcc371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# To make the output stable across other runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"training_farmer_income_models\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b69244-1c2f-4036-8d09-ad383a2bdf8a",
   "metadata": {},
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a5d8768-6623-4c9d-ba4e-bdff5846a549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_excel('Pearl Challenge data with dictionary.xlsx', sheet_name='TrainData')\n",
    "test_df = pd.read_excel('Pearl Challenge data with dictionary.xlsx', sheet_name='TestData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65f77383-6f22-47d8-ab3d-946791d82565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FarmerID</th>\n",
       "      <th>Zipcode</th>\n",
       "      <th>No_of_Active_Loan_In_Bureau</th>\n",
       "      <th>Avg_Disbursement_Amount_Bureau</th>\n",
       "      <th>Non_Agriculture_Income</th>\n",
       "      <th>Total_Land_For_Agriculture</th>\n",
       "      <th>K022-Proximity to nearest mandi (Km)</th>\n",
       "      <th>K022-Proximity to nearest railway (Km)</th>\n",
       "      <th>KO22-Village score based on socio-economic parameters (0 to 100)</th>\n",
       "      <th>K022-Seasonal Average Rainfall (mm)</th>\n",
       "      <th>R022-Seasonal Average Rainfall (mm)</th>\n",
       "      <th>K021-Seasonal Average Rainfall (mm)</th>\n",
       "      <th>R021-Seasonal Average Rainfall (mm)</th>\n",
       "      <th>R020-Seasonal Average Rainfall (mm)</th>\n",
       "      <th>Perc_of_house_with_6plus_room</th>\n",
       "      <th>Women_15_19_Mothers_or_Pregnant_at_time_of_survey</th>\n",
       "      <th>perc_of_pop_living_in_hh_electricity</th>\n",
       "      <th>perc_Households_with_Pucca_House_That_Has_More_Than_3_Rooms</th>\n",
       "      <th>mat_roof_Metal_GI_Asbestos_sheets</th>\n",
       "      <th>perc_of_Wall_material_with_Burnt_brick</th>\n",
       "      <th>Households_with_improved_Sanitation_Facility</th>\n",
       "      <th>perc_Households_do_not_have_KCC_With_The_Credit_Limit_Of_50k</th>\n",
       "      <th>K022-Total Geographical Area (in Hectares)-</th>\n",
       "      <th>K022-Net Agri area (in Ha)-</th>\n",
       "      <th>K022-Net Agri area (% of total geog area)-</th>\n",
       "      <th>Kharif Seasons  Irrigated area in 2022</th>\n",
       "      <th>Kharif Seasons  Cropping density in 2022</th>\n",
       "      <th>Kharif Seasons  Agricultural performance in 2022</th>\n",
       "      <th>Kharif Seasons  Agricultural Score in 2022</th>\n",
       "      <th>Kharif Seasons  Seasonal average groundwater thickness (cm) in 2022</th>\n",
       "      <th>Kharif Seasons  Seasonal average groundwater replenishment rate (cm) in 2022</th>\n",
       "      <th>Rabi Seasons  Season Irrigated area in 2022</th>\n",
       "      <th>Rabi Seasons Cropping density in 2022</th>\n",
       "      <th>Rabi Seasons Agricultural performance in 2022</th>\n",
       "      <th>Rabi Seasons Agricultural Score in 2022</th>\n",
       "      <th>Rabi Seasons Seasonal average groundwater thickness (cm) in 2022</th>\n",
       "      <th>Rabi Seasons Seasonal average groundwater replenishment rate (cm) in 2022</th>\n",
       "      <th>Rabi Seasons Kharif Season Irrigated area in 2021</th>\n",
       "      <th>Rabi Seasons Cropping density in 2021</th>\n",
       "      <th>Rabi Seasons Agricultural performance in 2021</th>\n",
       "      <th>Rabi Seasons Agricultural Score in 2021</th>\n",
       "      <th>Rabi Seasons Seasonal average groundwater thickness (cm) in 2021</th>\n",
       "      <th>Rabi Seasons Seasonal average groundwater replenishment rate (cm) in 2021</th>\n",
       "      <th>Kharif Seasons Kharif Season Irrigated area in 2021</th>\n",
       "      <th>Kharif Seasons Cropping density in 2021</th>\n",
       "      <th>Kharif Seasons Agricultural performance in 2021</th>\n",
       "      <th>Kharif Seasons Agricultural Score in 2021</th>\n",
       "      <th>Kharif Seasons Seasonal average groundwater thickness (cm) in 2021</th>\n",
       "      <th>Kharif Seasons Seasonal average groundwater replenishment rate (cm) in 2021</th>\n",
       "      <th>Kharif Seasons Kharif Season Irrigated area in 2020</th>\n",
       "      <th>Kharif Seasons Cropping density in 2020</th>\n",
       "      <th>Kharif Seasons Agricultural performance in 2020</th>\n",
       "      <th>Kharif Seasons Agricultural Score in 2020</th>\n",
       "      <th>Kharif Seasons Seasonal average groundwater thickness (cm) in 2020</th>\n",
       "      <th>Kharif Seasons Seasonal average groundwater replenishment rate (cm) in 2020</th>\n",
       "      <th>Rabi Seasons Kharif Season Irrigated area in 2020</th>\n",
       "      <th>Rabi Seasons Cropping density in 2020</th>\n",
       "      <th>Rabi Seasons Agricultural performance in 2020</th>\n",
       "      <th>Rabi Seasons Agricultural Score in 2020</th>\n",
       "      <th>Rabi Seasons Seasonal average groundwater thickness (cm) in 2020</th>\n",
       "      <th>Rabi Seasons Seasonal average groundwater replenishment rate (cm) in 2020</th>\n",
       "      <th>Night light index</th>\n",
       "      <th>Village score based on socio-economic parameters (Non normalised)</th>\n",
       "      <th>Village score based on socio-economic parameters (0 to 100)</th>\n",
       "      <th>Land Holding Index source (Total Agri Area/ no of people)</th>\n",
       "      <th>Road density (Km/ SqKm)</th>\n",
       "      <th>Target_Variable/Total Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.797000e+04</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>2.718000e+04</td>\n",
       "      <td>4.797000e+04</td>\n",
       "      <td>47899.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47802.000000</td>\n",
       "      <td>47802.000000</td>\n",
       "      <td>47802.000000</td>\n",
       "      <td>47802.000000</td>\n",
       "      <td>47802.000000</td>\n",
       "      <td>47802.000000</td>\n",
       "      <td>47802.000000</td>\n",
       "      <td>47802.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.00000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>47970.000000</td>\n",
       "      <td>4.797000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.488529e+15</td>\n",
       "      <td>486844.412529</td>\n",
       "      <td>1.780196</td>\n",
       "      <td>2.463834e+05</td>\n",
       "      <td>2.806439e+05</td>\n",
       "      <td>9.952977</td>\n",
       "      <td>11.502610</td>\n",
       "      <td>15.107913</td>\n",
       "      <td>37.387913</td>\n",
       "      <td>1394.703558</td>\n",
       "      <td>139.138688</td>\n",
       "      <td>930.843286</td>\n",
       "      <td>141.450563</td>\n",
       "      <td>184.277404</td>\n",
       "      <td>1.703530</td>\n",
       "      <td>7.129813</td>\n",
       "      <td>98.044752</td>\n",
       "      <td>5.472353</td>\n",
       "      <td>17.108636</td>\n",
       "      <td>39.974012</td>\n",
       "      <td>69.694892</td>\n",
       "      <td>3.414701</td>\n",
       "      <td>1343.381155</td>\n",
       "      <td>832.049711</td>\n",
       "      <td>64.111285</td>\n",
       "      <td>52.098883</td>\n",
       "      <td>56.66163</td>\n",
       "      <td>24.622157</td>\n",
       "      <td>26.630210</td>\n",
       "      <td>88.320294</td>\n",
       "      <td>25.338932</td>\n",
       "      <td>57.541951</td>\n",
       "      <td>47.554242</td>\n",
       "      <td>20.794567</td>\n",
       "      <td>23.078389</td>\n",
       "      <td>84.635742</td>\n",
       "      <td>17.367280</td>\n",
       "      <td>65.554164</td>\n",
       "      <td>45.361299</td>\n",
       "      <td>22.187941</td>\n",
       "      <td>32.662377</td>\n",
       "      <td>83.486760</td>\n",
       "      <td>20.997763</td>\n",
       "      <td>57.541951</td>\n",
       "      <td>47.554242</td>\n",
       "      <td>15.875101</td>\n",
       "      <td>19.844924</td>\n",
       "      <td>84.688148</td>\n",
       "      <td>22.076523</td>\n",
       "      <td>61.821247</td>\n",
       "      <td>43.171065</td>\n",
       "      <td>21.059823</td>\n",
       "      <td>26.278660</td>\n",
       "      <td>86.241283</td>\n",
       "      <td>26.136203</td>\n",
       "      <td>65.124157</td>\n",
       "      <td>46.260260</td>\n",
       "      <td>23.828276</td>\n",
       "      <td>25.177085</td>\n",
       "      <td>84.655071</td>\n",
       "      <td>21.216232</td>\n",
       "      <td>0.930509</td>\n",
       "      <td>24.762485</td>\n",
       "      <td>37.387913</td>\n",
       "      <td>0.683805</td>\n",
       "      <td>2.850370</td>\n",
       "      <td>1.222255e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.602973e+15</td>\n",
       "      <td>120222.366317</td>\n",
       "      <td>2.733938</td>\n",
       "      <td>7.111157e+05</td>\n",
       "      <td>1.707941e+06</td>\n",
       "      <td>6.903007</td>\n",
       "      <td>7.515967</td>\n",
       "      <td>13.984214</td>\n",
       "      <td>7.484105</td>\n",
       "      <td>424.028028</td>\n",
       "      <td>109.354315</td>\n",
       "      <td>253.501524</td>\n",
       "      <td>78.434748</td>\n",
       "      <td>125.439381</td>\n",
       "      <td>1.744805</td>\n",
       "      <td>4.460903</td>\n",
       "      <td>2.926635</td>\n",
       "      <td>4.124985</td>\n",
       "      <td>20.764845</td>\n",
       "      <td>19.388440</td>\n",
       "      <td>11.993306</td>\n",
       "      <td>4.031244</td>\n",
       "      <td>1335.892566</td>\n",
       "      <td>866.949345</td>\n",
       "      <td>18.739129</td>\n",
       "      <td>17.773827</td>\n",
       "      <td>18.23331</td>\n",
       "      <td>12.852373</td>\n",
       "      <td>8.738836</td>\n",
       "      <td>15.535760</td>\n",
       "      <td>7.232706</td>\n",
       "      <td>28.545970</td>\n",
       "      <td>27.162464</td>\n",
       "      <td>11.846883</td>\n",
       "      <td>8.817123</td>\n",
       "      <td>14.894818</td>\n",
       "      <td>6.440266</td>\n",
       "      <td>27.085117</td>\n",
       "      <td>25.262738</td>\n",
       "      <td>11.232275</td>\n",
       "      <td>10.559737</td>\n",
       "      <td>14.481376</td>\n",
       "      <td>7.731492</td>\n",
       "      <td>28.545970</td>\n",
       "      <td>27.162464</td>\n",
       "      <td>13.121163</td>\n",
       "      <td>7.727002</td>\n",
       "      <td>15.032286</td>\n",
       "      <td>6.576946</td>\n",
       "      <td>29.331604</td>\n",
       "      <td>25.453386</td>\n",
       "      <td>14.264316</td>\n",
       "      <td>10.667087</td>\n",
       "      <td>15.759384</td>\n",
       "      <td>8.198100</td>\n",
       "      <td>29.411770</td>\n",
       "      <td>26.216106</td>\n",
       "      <td>12.140449</td>\n",
       "      <td>9.083251</td>\n",
       "      <td>14.777610</td>\n",
       "      <td>7.074566</td>\n",
       "      <td>0.058456</td>\n",
       "      <td>4.617983</td>\n",
       "      <td>7.484105</td>\n",
       "      <td>1.980722</td>\n",
       "      <td>6.126023</td>\n",
       "      <td>2.073935e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000336e+15</td>\n",
       "      <td>122103.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.177678</td>\n",
       "      <td>530.370000</td>\n",
       "      <td>17.490000</td>\n",
       "      <td>369.650000</td>\n",
       "      <td>16.380000</td>\n",
       "      <td>19.140000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>68.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.570000</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>20.340000</td>\n",
       "      <td>6.350000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.870000</td>\n",
       "      <td>1.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.510000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>25.010000</td>\n",
       "      <td>1.160000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.300000</td>\n",
       "      <td>2.490000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.370000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>6.738640</td>\n",
       "      <td>8.177678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.900000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.210170e+15</td>\n",
       "      <td>445302.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.790262e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>32.980125</td>\n",
       "      <td>1057.190000</td>\n",
       "      <td>63.910000</td>\n",
       "      <td>750.600000</td>\n",
       "      <td>84.150000</td>\n",
       "      <td>100.910000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>4.520000</td>\n",
       "      <td>97.400000</td>\n",
       "      <td>2.730000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>26.240000</td>\n",
       "      <td>62.330000</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>492.380000</td>\n",
       "      <td>284.700000</td>\n",
       "      <td>54.180000</td>\n",
       "      <td>40.110000</td>\n",
       "      <td>45.33000</td>\n",
       "      <td>14.030000</td>\n",
       "      <td>20.700000</td>\n",
       "      <td>81.550000</td>\n",
       "      <td>21.190000</td>\n",
       "      <td>33.490000</td>\n",
       "      <td>25.360000</td>\n",
       "      <td>11.750000</td>\n",
       "      <td>16.850000</td>\n",
       "      <td>76.840000</td>\n",
       "      <td>13.870000</td>\n",
       "      <td>50.500000</td>\n",
       "      <td>25.157500</td>\n",
       "      <td>13.940000</td>\n",
       "      <td>26.400000</td>\n",
       "      <td>78.220000</td>\n",
       "      <td>15.840000</td>\n",
       "      <td>33.490000</td>\n",
       "      <td>25.360000</td>\n",
       "      <td>5.040000</td>\n",
       "      <td>14.150000</td>\n",
       "      <td>75.470000</td>\n",
       "      <td>17.970000</td>\n",
       "      <td>39.490000</td>\n",
       "      <td>22.320000</td>\n",
       "      <td>9.220000</td>\n",
       "      <td>18.530000</td>\n",
       "      <td>78.660000</td>\n",
       "      <td>21.200000</td>\n",
       "      <td>46.510000</td>\n",
       "      <td>25.980000</td>\n",
       "      <td>14.650000</td>\n",
       "      <td>18.820000</td>\n",
       "      <td>74.770000</td>\n",
       "      <td>17.410000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>22.042710</td>\n",
       "      <td>32.980125</td>\n",
       "      <td>0.156027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.200000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.488973e+15</td>\n",
       "      <td>483119.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.290978e+05</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>11.400000</td>\n",
       "      <td>37.776894</td>\n",
       "      <td>1375.800000</td>\n",
       "      <td>103.030000</td>\n",
       "      <td>933.790000</td>\n",
       "      <td>125.070000</td>\n",
       "      <td>142.800000</td>\n",
       "      <td>1.170000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>98.950000</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>8.190000</td>\n",
       "      <td>39.910000</td>\n",
       "      <td>71.300000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>901.070000</td>\n",
       "      <td>551.930000</td>\n",
       "      <td>69.780000</td>\n",
       "      <td>51.200000</td>\n",
       "      <td>57.47000</td>\n",
       "      <td>24.280000</td>\n",
       "      <td>26.900000</td>\n",
       "      <td>92.510000</td>\n",
       "      <td>25.570000</td>\n",
       "      <td>64.670000</td>\n",
       "      <td>47.850000</td>\n",
       "      <td>20.970000</td>\n",
       "      <td>23.470000</td>\n",
       "      <td>89.060000</td>\n",
       "      <td>16.060000</td>\n",
       "      <td>75.115000</td>\n",
       "      <td>46.840000</td>\n",
       "      <td>21.410000</td>\n",
       "      <td>33.600000</td>\n",
       "      <td>88.070000</td>\n",
       "      <td>21.610000</td>\n",
       "      <td>64.670000</td>\n",
       "      <td>47.850000</td>\n",
       "      <td>12.790000</td>\n",
       "      <td>19.530000</td>\n",
       "      <td>88.960000</td>\n",
       "      <td>22.240000</td>\n",
       "      <td>72.610000</td>\n",
       "      <td>42.580000</td>\n",
       "      <td>18.550000</td>\n",
       "      <td>26.910000</td>\n",
       "      <td>90.180000</td>\n",
       "      <td>26.760000</td>\n",
       "      <td>76.750000</td>\n",
       "      <td>45.990000</td>\n",
       "      <td>23.150000</td>\n",
       "      <td>26.410000</td>\n",
       "      <td>89.120000</td>\n",
       "      <td>20.370000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>25.002501</td>\n",
       "      <td>37.776894</td>\n",
       "      <td>0.304050</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>9.500000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.750388e+15</td>\n",
       "      <td>521109.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.840252e+05</td>\n",
       "      <td>2.500000e+05</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>15.975000</td>\n",
       "      <td>23.600000</td>\n",
       "      <td>41.990679</td>\n",
       "      <td>1687.490000</td>\n",
       "      <td>175.190000</td>\n",
       "      <td>1104.680000</td>\n",
       "      <td>180.540000</td>\n",
       "      <td>232.170000</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>8.660000</td>\n",
       "      <td>99.600000</td>\n",
       "      <td>6.850000</td>\n",
       "      <td>23.120000</td>\n",
       "      <td>51.577500</td>\n",
       "      <td>77.300000</td>\n",
       "      <td>4.710000</td>\n",
       "      <td>1778.790000</td>\n",
       "      <td>1077.940000</td>\n",
       "      <td>78.260000</td>\n",
       "      <td>64.030000</td>\n",
       "      <td>69.32000</td>\n",
       "      <td>34.130000</td>\n",
       "      <td>32.690000</td>\n",
       "      <td>98.110000</td>\n",
       "      <td>29.230000</td>\n",
       "      <td>82.710000</td>\n",
       "      <td>68.900000</td>\n",
       "      <td>29.190000</td>\n",
       "      <td>29.290000</td>\n",
       "      <td>94.370000</td>\n",
       "      <td>19.060000</td>\n",
       "      <td>87.037500</td>\n",
       "      <td>65.110000</td>\n",
       "      <td>29.137500</td>\n",
       "      <td>39.770000</td>\n",
       "      <td>93.180000</td>\n",
       "      <td>26.060000</td>\n",
       "      <td>82.710000</td>\n",
       "      <td>68.900000</td>\n",
       "      <td>24.260000</td>\n",
       "      <td>24.860000</td>\n",
       "      <td>94.670000</td>\n",
       "      <td>26.380000</td>\n",
       "      <td>86.470000</td>\n",
       "      <td>62.790000</td>\n",
       "      <td>31.270000</td>\n",
       "      <td>33.750000</td>\n",
       "      <td>97.010000</td>\n",
       "      <td>31.330000</td>\n",
       "      <td>87.980000</td>\n",
       "      <td>67.720000</td>\n",
       "      <td>32.310000</td>\n",
       "      <td>31.530000</td>\n",
       "      <td>95.380000</td>\n",
       "      <td>25.180000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>27.602570</td>\n",
       "      <td>41.990679</td>\n",
       "      <td>0.583235</td>\n",
       "      <td>3.420000</td>\n",
       "      <td>1.295000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.999402e+15</td>\n",
       "      <td>855117.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>8.000000e+07</td>\n",
       "      <td>7.128230e+07</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>44.200000</td>\n",
       "      <td>93.700000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2569.100000</td>\n",
       "      <td>733.760000</td>\n",
       "      <td>2239.360000</td>\n",
       "      <td>621.470000</td>\n",
       "      <td>936.980000</td>\n",
       "      <td>16.040000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>44.310000</td>\n",
       "      <td>88.230000</td>\n",
       "      <td>96.360000</td>\n",
       "      <td>97.100000</td>\n",
       "      <td>25.530000</td>\n",
       "      <td>30599.390000</td>\n",
       "      <td>8527.610000</td>\n",
       "      <td>96.700000</td>\n",
       "      <td>97.060000</td>\n",
       "      <td>96.35000</td>\n",
       "      <td>67.970000</td>\n",
       "      <td>55.860000</td>\n",
       "      <td>178.830000</td>\n",
       "      <td>52.060000</td>\n",
       "      <td>97.940000</td>\n",
       "      <td>97.970000</td>\n",
       "      <td>60.780000</td>\n",
       "      <td>50.960000</td>\n",
       "      <td>165.390000</td>\n",
       "      <td>50.670000</td>\n",
       "      <td>97.990000</td>\n",
       "      <td>97.960000</td>\n",
       "      <td>68.110000</td>\n",
       "      <td>68.200000</td>\n",
       "      <td>157.910000</td>\n",
       "      <td>53.770000</td>\n",
       "      <td>97.940000</td>\n",
       "      <td>97.970000</td>\n",
       "      <td>71.140000</td>\n",
       "      <td>51.180000</td>\n",
       "      <td>182.610000</td>\n",
       "      <td>47.790000</td>\n",
       "      <td>97.990000</td>\n",
       "      <td>97.960000</td>\n",
       "      <td>71.960000</td>\n",
       "      <td>59.370000</td>\n",
       "      <td>176.240000</td>\n",
       "      <td>62.690000</td>\n",
       "      <td>97.980000</td>\n",
       "      <td>97.590000</td>\n",
       "      <td>67.820000</td>\n",
       "      <td>57.620000</td>\n",
       "      <td>157.700000</td>\n",
       "      <td>56.880000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>63.396566</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>112.603896</td>\n",
       "      <td>198.550000</td>\n",
       "      <td>8.000000e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           FarmerID        Zipcode  ...   Road density (Km/ SqKm)  Target_Variable/Total Income\n",
       "count  4.797000e+04   47970.000000  ...              47970.000000                  4.797000e+04\n",
       "mean   5.488529e+15  486844.412529  ...                  2.850370                  1.222255e+06\n",
       "std    2.602973e+15  120222.366317  ...                  6.126023                  2.073935e+06\n",
       "min    1.000336e+15  122103.000000  ...                  0.000000                  2.900000e+04\n",
       "25%    3.210170e+15  445302.250000  ...                  0.000000                  7.200000e+05\n",
       "50%    5.488973e+15  483119.000000  ...                  0.460000                  9.500000e+05\n",
       "75%    7.750388e+15  521109.000000  ...                  3.420000                  1.295000e+06\n",
       "max    9.999402e+15  855117.000000  ...                198.550000                  8.000000e+07\n",
       "\n",
       "[8 rows x 67 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3db2ee5-03a3-479e-84c4-5b73883bc342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainData size: 47970\n",
      "TestData size: 9986\n"
     ]
    }
   ],
   "source": [
    "print(\"TrainData size:\",len(train_df))\n",
    "print(\"TestData size:\", len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a0d1d3-db04-4108-9b6c-4c70cbd3f17c",
   "metadata": {},
   "source": [
    "# Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7057f7d7-16ae-4be3-bd56-fd676d3a3fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['FarmerID', 'Zipcode', 'CITY', 'DISTRICT', 'VILLAGE', 'Location']\n",
    "target = 'Target_Variable/Total Income'\n",
    "X = train_df.drop(columns=[target] + drop_cols)\n",
    "y = train_df[target]\n",
    "\n",
    "X_test = test_df.drop(columns=[target] + drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2432ed44-4593-4ce8-ad22-7fd1dee3daf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cat_cols = X.select_dtypes(include='object').columns.tolist()\n",
    "num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_cols),\n",
    "        ('cat', categorical_transformer, cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30da2b27-5d12-4994-b7a7-cd9149a75cd4",
   "metadata": {},
   "source": [
    "# Building neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaff6b6-502c-4200-a9c8-7409a32e4500",
   "metadata": {},
   "source": [
    "## 1. Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c58388f5-a13e-4d5b-9d0b-3a7bb43456fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 5811745062912.0000 - mape: 93.7802 - val_loss: 4640217759744.0000 - val_mape: 31.7988\n",
      "Epoch 2/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 4012301877248.0000 - mape: 36.8082 - val_loss: 4334829436928.0000 - val_mape: 46.5302\n",
      "Epoch 3/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 3338294263808.0000 - mape: 47.9003 - val_loss: 4296961163264.0000 - val_mape: 49.0209\n",
      "Epoch 4/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 4475205451776.0000 - mape: 49.9652 - val_loss: 4265132949504.0000 - val_mape: 46.9450\n",
      "Epoch 5/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 3758292205568.0000 - mape: 48.4714 - val_loss: 4230997606400.0000 - val_mape: 48.2474\n",
      "Epoch 6/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 4526061912064.0000 - mape: 49.5967 - val_loss: 4198045319168.0000 - val_mape: 46.9132\n",
      "Epoch 7/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 4202643587072.0000 - mape: 47.9740 - val_loss: 4158597890048.0000 - val_mape: 48.0060\n",
      "Epoch 8/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 2843596292096.0000 - mape: 47.8024 - val_loss: 4117536440320.0000 - val_mape: 50.3828\n",
      "Epoch 9/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 3699457654784.0000 - mape: 49.5078 - val_loss: 4075092705280.0000 - val_mape: 47.4173\n",
      "Epoch 10/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 5006459142144.0000 - mape: 49.2613 - val_loss: 4028871737344.0000 - val_mape: 45.9891\n",
      "Epoch 11/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 3118267367424.0000 - mape: 46.3017 - val_loss: 3974281822208.0000 - val_mape: 47.3591\n",
      "Epoch 12/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 4084253589504.0000 - mape: 47.8107 - val_loss: 3919146909696.0000 - val_mape: 45.3172\n",
      "Epoch 13/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 3477653946368.0000 - mape: 45.6180 - val_loss: 3854957019136.0000 - val_mape: 45.6150\n",
      "Epoch 14/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 3772633317376.0000 - mape: 46.7251 - val_loss: 3788665782272.0000 - val_mape: 44.2548\n",
      "Epoch 15/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 3967315345408.0000 - mape: 45.2343 - val_loss: 3712341508096.0000 - val_mape: 45.1940\n",
      "Epoch 16/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 3527931068416.0000 - mape: 45.7212 - val_loss: 3631573368832.0000 - val_mape: 44.3598\n",
      "Epoch 17/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 3353182470144.0000 - mape: 45.6316 - val_loss: 3543314726912.0000 - val_mape: 42.9150\n",
      "Epoch 18/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 3913771384832.0000 - mape: 43.8771 - val_loss: 3449340559360.0000 - val_mape: 43.1307\n",
      "Epoch 19/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 3508710932480.0000 - mape: 44.2603 - val_loss: 3349964652544.0000 - val_mape: 42.5897\n",
      "Epoch 20/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 4113977049088.0000 - mape: 44.6829 - val_loss: 3239131480064.0000 - val_mape: 41.8756\n",
      "Epoch 21/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 2795650154496.0000 - mape: 43.0081 - val_loss: 3121196564480.0000 - val_mape: 43.1407\n",
      "Epoch 22/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 3522787278848.0000 - mape: 43.4745 - val_loss: 3003258765312.0000 - val_mape: 42.3467\n",
      "Epoch 23/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 3439935881216.0000 - mape: 43.6872 - val_loss: 2878605361152.0000 - val_mape: 42.3858\n",
      "Epoch 24/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 2753417445376.0000 - mape: 42.8282 - val_loss: 2739001360384.0000 - val_mape: 41.7764\n",
      "Epoch 25/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 2974491607040.0000 - mape: 42.5448 - val_loss: 2599568015360.0000 - val_mape: 41.5533\n",
      "Epoch 26/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 2808401887232.0000 - mape: 42.0760 - val_loss: 2453891448832.0000 - val_mape: 41.2003\n",
      "Epoch 27/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 1999313305600.0000 - mape: 40.9238 - val_loss: 2298764591104.0000 - val_mape: 40.2509\n",
      "Epoch 28/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 2175166316544.0000 - mape: 41.5489 - val_loss: 2144556154880.0000 - val_mape: 40.5332\n",
      "Epoch 29/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 1929236185088.0000 - mape: 40.5556 - val_loss: 1987208675328.0000 - val_mape: 39.7091\n",
      "Epoch 30/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 1999265464320.0000 - mape: 40.2100 - val_loss: 1833475637248.0000 - val_mape: 39.2045\n",
      "Epoch 31/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 1838404599808.0000 - mape: 39.5373 - val_loss: 1673818669056.0000 - val_mape: 37.8640\n",
      "Epoch 32/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 1475430187008.0000 - mape: 39.2418 - val_loss: 1516279169024.0000 - val_mape: 37.9255\n",
      "Epoch 33/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 1211422998528.0000 - mape: 38.6458 - val_loss: 1366157819904.0000 - val_mape: 37.8384\n",
      "Epoch 34/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 1141047820288.0000 - mape: 37.9149 - val_loss: 1222272614400.0000 - val_mape: 35.8599\n",
      "Epoch 35/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 1113531088896.0000 - mape: 37.2725 - val_loss: 1076824244224.0000 - val_mape: 35.6470\n",
      "Epoch 36/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 1158227951616.0000 - mape: 37.6642 - val_loss: 951500865536.0000 - val_mape: 35.3648\n",
      "Epoch 37/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 1007891054592.0000 - mape: 36.2097 - val_loss: 833404207104.0000 - val_mape: 34.8819\n",
      "Epoch 38/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 851792297984.0000 - mape: 35.7836 - val_loss: 723946176512.0000 - val_mape: 33.6425\n",
      "Epoch 39/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 635656339456.0000 - mape: 34.6694 - val_loss: 630815784960.0000 - val_mape: 32.9695\n",
      "Epoch 40/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 604734554112.0000 - mape: 33.5326 - val_loss: 553500278784.0000 - val_mape: 33.0809\n",
      "Epoch 41/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - loss: 547850223616.0000 - mape: 33.3884 - val_loss: 489868689408.0000 - val_mape: 32.2545\n",
      "Epoch 42/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 568360370176.0000 - mape: 32.8213 - val_loss: 450508619776.0000 - val_mape: 30.9928\n",
      "Epoch 43/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 529681612800.0000 - mape: 32.4857 - val_loss: 418488647680.0000 - val_mape: 30.7095\n",
      "Epoch 44/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 507607711744.0000 - mape: 32.2301 - val_loss: 399187116032.0000 - val_mape: 30.2883\n",
      "Epoch 45/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 430298267648.0000 - mape: 31.4398 - val_loss: 392323334144.0000 - val_mape: 30.2156\n",
      "Epoch 46/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 463918071808.0000 - mape: 31.1249 - val_loss: 390134071296.0000 - val_mape: 30.1106\n",
      "Epoch 47/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 435859718144.0000 - mape: 31.0703 - val_loss: 386781577216.0000 - val_mape: 28.9315\n",
      "Epoch 48/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 434962595840.0000 - mape: 30.6949 - val_loss: 381919264768.0000 - val_mape: 29.8731\n",
      "Epoch 49/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 437902573568.0000 - mape: 30.7529 - val_loss: 379053375488.0000 - val_mape: 29.8862\n",
      "Epoch 50/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 439347380224.0000 - mape: 30.8364 - val_loss: 376781799424.0000 - val_mape: 29.4685\n",
      "Epoch 51/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 454094618624.0000 - mape: 30.7624 - val_loss: 374580445184.0000 - val_mape: 30.3288\n",
      "Epoch 52/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 525974962176.0000 - mape: 31.2413 - val_loss: 373349974016.0000 - val_mape: 28.8422\n",
      "Epoch 53/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 443466973184.0000 - mape: 30.1793 - val_loss: 372899151872.0000 - val_mape: 29.6048\n",
      "Epoch 54/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 468397424640.0000 - mape: 30.5885 - val_loss: 372935557120.0000 - val_mape: 28.7613\n",
      "Epoch 55/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 448909705216.0000 - mape: 29.8544 - val_loss: 372186054656.0000 - val_mape: 29.9520\n",
      "Epoch 56/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 475736440832.0000 - mape: 30.3680 - val_loss: 371276546048.0000 - val_mape: 29.5079\n",
      "Epoch 57/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 422310117376.0000 - mape: 30.2977 - val_loss: 370630492160.0000 - val_mape: 29.4027\n",
      "Epoch 58/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 456928788480.0000 - mape: 29.7364 - val_loss: 369836195840.0000 - val_mape: 29.4712\n",
      "Epoch 59/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 509078372352.0000 - mape: 29.9860 - val_loss: 370156732416.0000 - val_mape: 30.2445\n",
      "Epoch 60/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 434068520960.0000 - mape: 30.1422 - val_loss: 370786992128.0000 - val_mape: 29.9064\n",
      "Epoch 61/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 455951024128.0000 - mape: 30.0884 - val_loss: 370931236864.0000 - val_mape: 28.6936\n",
      "Epoch 62/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 510633803776.0000 - mape: 30.0178 - val_loss: 370369560576.0000 - val_mape: 29.3631\n",
      "Epoch 63/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 468920893440.0000 - mape: 29.5029 - val_loss: 370555289600.0000 - val_mape: 29.2669\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Input dimension\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_dim=input_dim),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mape'])\n",
    "\n",
    "# Train with early stopping\n",
    "es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[es],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9392246-283f-4ba3-b284-842d785e9743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_scaled = model.predict(X_val)\n",
    "\n",
    "# Inverse transform both prediction and true values\n",
    "y_pred = y_scaler.inverse_transform(y_pred_scaled)\n",
    "y_true = y_scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Compute MAPE\n",
    "real_mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "print(f\"Real Test MAPE: {real_mape:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d56934-f56d-4879-8941-e834b1b3dda0",
   "metadata": {},
   "source": [
    "## 2. Overkill NN (using BatchNormalization+Dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55776279-07c4-404b-85fc-7a11fe3c1a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "def build_better_nn(input_dim):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    \n",
    "    x = Dense(512, activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    outputs = Dense(1)(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='mse',\n",
    "                  metrics=['mape'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "790f40c9-1300-4d82-b7c8-ca489fb79a21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 16ms/step - loss: 5915786870784.0000 - mape: 99.9997 - val_loss: 5886010982400.0000 - val_mape: 99.9985 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 6683652259840.0000 - mape: 99.9909 - val_loss: 5884581773312.0000 - val_mape: 99.9847 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5648730292224.0000 - mape: 99.9685 - val_loss: 5882362462208.0000 - val_mape: 99.9501 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 6393287409664.0000 - mape: 99.9340 - val_loss: 5879887298560.0000 - val_mape: 99.9100 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 6773626372096.0000 - mape: 99.8887 - val_loss: 5876894138368.0000 - val_mape: 99.8560 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5893537136640.0000 - mape: 99.8333 - val_loss: 5872975085568.0000 - val_mape: 99.7898 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5658804486144.0000 - mape: 99.7680 - val_loss: 5868413779968.0000 - val_mape: 99.7150 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5385901572096.0000 - mape: 99.6941 - val_loss: 5863225950208.0000 - val_mape: 99.6412 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 6304873578496.0000 - mape: 99.6117 - val_loss: 5855659425792.0000 - val_mape: 99.5431 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5060135747584.0000 - mape: 99.5231 - val_loss: 5852267806720.0000 - val_mape: 99.4212 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5818574962688.0000 - mape: 99.4217 - val_loss: 5846168764416.0000 - val_mape: 99.3405 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5267419299840.0000 - mape: 99.3139 - val_loss: 5842596265984.0000 - val_mape: 99.1914 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 6739280265216.0000 - mape: 99.1977 - val_loss: 5814325608448.0000 - val_mape: 99.0824 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5697040285696.0000 - mape: 99.0798 - val_loss: 5820548382720.0000 - val_mape: 98.8376 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 7062397386752.0000 - mape: 98.9428 - val_loss: 5804408176640.0000 - val_mape: 98.7507 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5628099559424.0000 - mape: 98.8081 - val_loss: 5793769324544.0000 - val_mape: 98.6418 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5031163068416.0000 - mape: 98.6764 - val_loss: 5802940170240.0000 - val_mape: 98.4694 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5084991717376.0000 - mape: 98.5220 - val_loss: 5785496584192.0000 - val_mape: 98.3100 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 4921889914880.0000 - mape: 98.3549 - val_loss: 5775541927936.0000 - val_mape: 98.1955 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5206253240320.0000 - mape: 98.1805 - val_loss: 5766430326784.0000 - val_mape: 98.0005 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 4438338043904.0000 - mape: 98.0168 - val_loss: 5775677718528.0000 - val_mape: 97.8292 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 6291583401984.0000 - mape: 97.8363 - val_loss: 5727311626240.0000 - val_mape: 97.6236 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5809813061632.0000 - mape: 97.6247 - val_loss: 5690195181568.0000 - val_mape: 97.4631 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5557376253952.0000 - mape: 97.4431 - val_loss: 5694354882560.0000 - val_mape: 97.2007 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 4616098414592.0000 - mape: 97.2312 - val_loss: 5691428306944.0000 - val_mape: 97.0526 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 6252529188864.0000 - mape: 97.0200 - val_loss: 5657018236928.0000 - val_mape: 96.8089 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 4890001670144.0000 - mape: 96.8207 - val_loss: 5678182170624.0000 - val_mape: 96.5922 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5279624724480.0000 - mape: 96.5734 - val_loss: 5642258481152.0000 - val_mape: 96.3116 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 6362918551552.0000 - mape: 96.3686 - val_loss: 5645190823936.0000 - val_mape: 96.1113 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5314127069184.0000 - mape: 96.1054 - val_loss: 5633641283584.0000 - val_mape: 95.9704 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 6485566291968.0000 - mape: 95.8601 - val_loss: 5587933331456.0000 - val_mape: 95.6621 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5865958539264.0000 - mape: 95.5924 - val_loss: 5572145971200.0000 - val_mape: 95.3117 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 6370338275328.0000 - mape: 95.3373 - val_loss: 5577430269952.0000 - val_mape: 95.1863 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 6101535293440.0000 - mape: 95.0881 - val_loss: 5559474454528.0000 - val_mape: 94.7592 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5483790860288.0000 - mape: 94.7750 - val_loss: 5568268337152.0000 - val_mape: 94.4877 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 6158371782656.0000 - mape: 94.5312 - val_loss: 5559228039168.0000 - val_mape: 94.3032 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 4674325315584.0000 - mape: 94.2490 - val_loss: 5488758489088.0000 - val_mape: 93.8652 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 4375340908544.0000 - mape: 93.9621 - val_loss: 5447668465664.0000 - val_mape: 93.6469 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5269282619392.0000 - mape: 93.6236 - val_loss: 5453151993856.0000 - val_mape: 93.2848 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5250665152512.0000 - mape: 93.3233 - val_loss: 5523538706432.0000 - val_mape: 93.0455 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 4798383915008.0000 - mape: 93.0227 - val_loss: 5469674930176.0000 - val_mape: 92.5332 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5866359095296.0000 - mape: 92.6601 - val_loss: 5519004139520.0000 - val_mape: 92.2381 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 6147101163520.0000 - mape: 92.3341 - val_loss: 5362833424384.0000 - val_mape: 92.0195 - learning_rate: 0.0010\n",
      "Epoch 44/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5402540900352.0000 - mape: 92.0586 - val_loss: 5427022004224.0000 - val_mape: 91.5587 - learning_rate: 0.0010\n",
      "Epoch 45/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5129185001472.0000 - mape: 91.6830 - val_loss: 5407671058432.0000 - val_mape: 91.1631 - learning_rate: 0.0010\n",
      "Epoch 46/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5601448427520.0000 - mape: 91.3130 - val_loss: 5368308563968.0000 - val_mape: 90.8960 - learning_rate: 0.0010\n",
      "Epoch 47/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5957939101696.0000 - mape: 90.9996 - val_loss: 5306064044032.0000 - val_mape: 90.4775 - learning_rate: 0.0010\n",
      "Epoch 48/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 6032288382976.0000 - mape: 90.6053 - val_loss: 5290841341952.0000 - val_mape: 90.2019 - learning_rate: 0.0010\n",
      "Epoch 49/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 4497792303104.0000 - mape: 90.2681 - val_loss: 5248682295296.0000 - val_mape: 89.7527 - learning_rate: 0.0010\n",
      "Epoch 50/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5041851727872.0000 - mape: 89.9187 - val_loss: 5295152562176.0000 - val_mape: 89.3614 - learning_rate: 0.0010\n",
      "Epoch 51/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 6575739633664.0000 - mape: 89.5219 - val_loss: 5253230493696.0000 - val_mape: 88.9068 - learning_rate: 0.0010\n",
      "Epoch 52/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5004760449024.0000 - mape: 89.0765 - val_loss: 5297253384192.0000 - val_mape: 88.4232 - learning_rate: 0.0010\n",
      "Epoch 53/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 4783543943168.0000 - mape: 88.6949 - val_loss: 5178416693248.0000 - val_mape: 88.1248 - learning_rate: 0.0010\n",
      "Epoch 54/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 6046058807296.0000 - mape: 88.3315 - val_loss: 5109758558208.0000 - val_mape: 87.7049 - learning_rate: 0.0010\n",
      "Epoch 55/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5506867396608.0000 - mape: 87.8893 - val_loss: 5186286256128.0000 - val_mape: 87.2739 - learning_rate: 0.0010\n",
      "Epoch 56/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 6369462190080.0000 - mape: 87.4323 - val_loss: 5107736903680.0000 - val_mape: 86.7047 - learning_rate: 0.0010\n",
      "Epoch 57/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5246270046208.0000 - mape: 87.0370 - val_loss: 5069564542976.0000 - val_mape: 86.3985 - learning_rate: 0.0010\n",
      "Epoch 58/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5687826972672.0000 - mape: 86.6428 - val_loss: 5075822968832.0000 - val_mape: 85.7972 - learning_rate: 0.0010\n",
      "Epoch 59/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 4623300558848.0000 - mape: 86.3066 - val_loss: 5140878196736.0000 - val_mape: 85.4455 - learning_rate: 0.0010\n",
      "Epoch 60/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 4698835255296.0000 - mape: 85.7886 - val_loss: 4997006229504.0000 - val_mape: 85.0033 - learning_rate: 0.0010\n",
      "Epoch 61/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5637261492224.0000 - mape: 85.2898 - val_loss: 4924389195776.0000 - val_mape: 84.7991 - learning_rate: 0.0010\n",
      "Epoch 62/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5833801859072.0000 - mape: 84.8121 - val_loss: 4848856072192.0000 - val_mape: 84.1729 - learning_rate: 0.0010\n",
      "Epoch 63/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 4769594212352.0000 - mape: 84.4139 - val_loss: 4897704509440.0000 - val_mape: 83.4169 - learning_rate: 0.0010\n",
      "Epoch 64/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5554919964672.0000 - mape: 83.9826 - val_loss: 4898963849216.0000 - val_mape: 83.0202 - learning_rate: 0.0010\n",
      "Epoch 65/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5337123389440.0000 - mape: 83.4995 - val_loss: 4777825009664.0000 - val_mape: 82.7353 - learning_rate: 0.0010\n",
      "Epoch 66/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 4444117794816.0000 - mape: 83.0101 - val_loss: 4926688722944.0000 - val_mape: 81.5551 - learning_rate: 0.0010\n",
      "Epoch 67/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5959857995776.0000 - mape: 82.5098 - val_loss: 4628161757184.0000 - val_mape: 81.5137 - learning_rate: 0.0010\n",
      "Epoch 68/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5177555288064.0000 - mape: 82.0438 - val_loss: 4837419253760.0000 - val_mape: 80.9949 - learning_rate: 0.0010\n",
      "Epoch 69/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 5550665367552.0000 - mape: 81.4895 - val_loss: 4806269206528.0000 - val_mape: 80.3900 - learning_rate: 0.0010\n",
      "Epoch 70/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 4935138148352.0000 - mape: 81.0533 - val_loss: 4741722013696.0000 - val_mape: 79.9781 - learning_rate: 0.0010\n",
      "Epoch 71/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 4481134100480.0000 - mape: 80.5703 - val_loss: 4769987952640.0000 - val_mape: 79.6083 - learning_rate: 0.0010\n",
      "Epoch 72/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 4271493873664.0000 - mape: 80.0234 - val_loss: 4831165022208.0000 - val_mape: 78.4612 - learning_rate: 0.0010\n",
      "Epoch 73/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 4741558960128.0000 - mape: 79.4770 - val_loss: 4705091584000.0000 - val_mape: 78.6073 - learning_rate: 5.0000e-04\n",
      "Epoch 74/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 5163134222336.0000 - mape: 79.2276 - val_loss: 4662617440256.0000 - val_mape: 78.3633 - learning_rate: 5.0000e-04\n",
      "Epoch 75/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 3702965141504.0000 - mape: 79.0958 - val_loss: 4801666482176.0000 - val_mape: 77.8766 - learning_rate: 5.0000e-04\n",
      "Epoch 76/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 4761016860672.0000 - mape: 78.8076 - val_loss: 4551294844928.0000 - val_mape: 77.8865 - learning_rate: 5.0000e-04\n",
      "Epoch 77/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 4694640427008.0000 - mape: 78.5082 - val_loss: 4630692495360.0000 - val_mape: 77.5459 - learning_rate: 5.0000e-04\n",
      "Epoch 78/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 4505249251328.0000 - mape: 78.3423 - val_loss: 4663494574080.0000 - val_mape: 77.0555 - learning_rate: 5.0000e-04\n",
      "Epoch 79/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 4188624650240.0000 - mape: 78.0163 - val_loss: 4718586757120.0000 - val_mape: 77.1155 - learning_rate: 5.0000e-04\n",
      "Epoch 80/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 4897476968448.0000 - mape: 77.8055 - val_loss: 4714330062848.0000 - val_mape: 76.6213 - learning_rate: 5.0000e-04\n",
      "Epoch 81/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 5107780943872.0000 - mape: 77.4306 - val_loss: 4679548272640.0000 - val_mape: 76.3016 - learning_rate: 5.0000e-04\n",
      "Epoch 82/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 4827776024576.0000 - mape: 77.2444 - val_loss: 4539775713280.0000 - val_mape: 76.1763 - learning_rate: 2.5000e-04\n",
      "Epoch 83/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 6101352841216.0000 - mape: 77.1717 - val_loss: 4475991359488.0000 - val_mape: 76.2771 - learning_rate: 2.5000e-04\n",
      "Epoch 84/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 5797398446080.0000 - mape: 76.9200 - val_loss: 4439980638208.0000 - val_mape: 76.0944 - learning_rate: 2.5000e-04\n",
      "Epoch 85/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 4261269209088.0000 - mape: 76.8227 - val_loss: 4643455238144.0000 - val_mape: 75.5746 - learning_rate: 2.5000e-04\n",
      "Epoch 86/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 5430771187712.0000 - mape: 76.6025 - val_loss: 4543814303744.0000 - val_mape: 75.7724 - learning_rate: 2.5000e-04\n",
      "Epoch 87/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 5756346695680.0000 - mape: 76.5141 - val_loss: 4455355908096.0000 - val_mape: 75.5491 - learning_rate: 2.5000e-04\n",
      "Epoch 88/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 4652541149184.0000 - mape: 76.4152 - val_loss: 4478470193152.0000 - val_mape: 75.4947 - learning_rate: 2.5000e-04\n",
      "Epoch 89/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 5142746759168.0000 - mape: 76.1918 - val_loss: 4526108573696.0000 - val_mape: 75.1251 - learning_rate: 2.5000e-04\n",
      "Epoch 90/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 6016164429824.0000 - mape: 76.2120 - val_loss: 4529942167552.0000 - val_mape: 75.0419 - learning_rate: 1.2500e-04\n",
      "Epoch 91/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 5543656685568.0000 - mape: 76.2285 - val_loss: 4550738051072.0000 - val_mape: 75.0411 - learning_rate: 1.2500e-04\n",
      "Epoch 92/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 5504011075584.0000 - mape: 76.0359 - val_loss: 4486021513216.0000 - val_mape: 74.9466 - learning_rate: 1.2500e-04\n",
      "Epoch 93/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 5173692334080.0000 - mape: 75.9163 - val_loss: 4500013711360.0000 - val_mape: 74.9624 - learning_rate: 1.2500e-04\n",
      "Epoch 94/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 4429512179712.0000 - mape: 75.9856 - val_loss: 4576868040704.0000 - val_mape: 74.8147 - learning_rate: 1.2500e-04\n",
      "Epoch 95/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 4787845201920.0000 - mape: 75.8024 - val_loss: 4517529649152.0000 - val_mape: 74.8549 - learning_rate: 6.2500e-05\n",
      "Epoch 96/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 4944553836544.0000 - mape: 75.9528 - val_loss: 4631453237248.0000 - val_mape: 74.6876 - learning_rate: 6.2500e-05\n",
      "Epoch 97/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 5225506144256.0000 - mape: 75.8942 - val_loss: 4495228010496.0000 - val_mape: 74.6905 - learning_rate: 6.2500e-05\n",
      "Epoch 98/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 5265608933376.0000 - mape: 75.6753 - val_loss: 4612945870848.0000 - val_mape: 74.6941 - learning_rate: 6.2500e-05\n",
      "Epoch 99/200\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 4459679711232.0000 - mape: 75.7091 - val_loss: 4511137529856.0000 - val_mape: 74.7994 - learning_rate: 6.2500e-05\n"
     ]
    }
   ],
   "source": [
    "model = build_better_nn(X_train.shape[1])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop, lr_scheduler],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f8a2dc-2910-41ec-b8f2-55f3121d28a7",
   "metadata": {},
   "source": [
    "## 3. A bit more balanced NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8938fa9a-dd5f-4010-a6db-279d5d616be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def build_tuned_nn(input_dim):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    \n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    outputs = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='mse',\n",
    "                  metrics=['mape'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7306f528-69f2-4593-b9a7-1bc434c69633",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 5437347332096.0000 - mape: 93.1649 - val_loss: 4591176908800.0000 - val_mape: 30.3239\n",
      "Epoch 2/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 3878629146624.0000 - mape: 36.7078 - val_loss: 4326619086848.0000 - val_mape: 47.8752\n",
      "Epoch 3/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 3807391252480.0000 - mape: 47.9186 - val_loss: 4288770998272.0000 - val_mape: 49.5586\n",
      "Epoch 4/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 4164289822720.0000 - mape: 49.5149 - val_loss: 4252754509824.0000 - val_mape: 48.7693\n",
      "Epoch 5/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 5207862280192.0000 - mape: 49.5958 - val_loss: 4215910957056.0000 - val_mape: 47.3944\n",
      "Epoch 6/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 5106872877056.0000 - mape: 49.8017 - val_loss: 4177604640768.0000 - val_mape: 46.8257\n",
      "Epoch 7/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 4265179348992.0000 - mape: 47.2867 - val_loss: 4134202507264.0000 - val_mape: 48.4941\n",
      "Epoch 8/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 2930505940992.0000 - mape: 47.6074 - val_loss: 4087202185216.0000 - val_mape: 48.3630\n",
      "Epoch 9/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 4439563829248.0000 - mape: 48.4218 - val_loss: 4037568364544.0000 - val_mape: 47.0834\n",
      "Epoch 10/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 3063508107264.0000 - mape: 47.1716 - val_loss: 3979311316992.0000 - val_mape: 47.9375\n",
      "Epoch 11/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 4491564810240.0000 - mape: 48.3349 - val_loss: 3920990830592.0000 - val_mape: 44.9456\n",
      "Epoch 12/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 3148947914752.0000 - mape: 45.0941 - val_loss: 3846987841536.0000 - val_mape: 47.0572\n",
      "Epoch 13/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 4221813391360.0000 - mape: 47.2311 - val_loss: 3774289018880.0000 - val_mape: 43.9617\n",
      "Epoch 14/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 3676858220544.0000 - mape: 45.2454 - val_loss: 3690560225280.0000 - val_mape: 44.4383\n",
      "Epoch 15/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 3616651083776.0000 - mape: 45.3246 - val_loss: 3596704022528.0000 - val_mape: 44.9035\n",
      "Epoch 16/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 2940110897152.0000 - mape: 44.3118 - val_loss: 3493904777216.0000 - val_mape: 44.9140\n",
      "Epoch 17/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 3610694123520.0000 - mape: 45.0254 - val_loss: 3384824299520.0000 - val_mape: 44.3612\n",
      "Epoch 18/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 3399069204480.0000 - mape: 44.0692 - val_loss: 3267998253056.0000 - val_mape: 43.5161\n",
      "Epoch 19/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 3200646381568.0000 - mape: 43.5690 - val_loss: 3141889687552.0000 - val_mape: 43.6745\n",
      "Epoch 20/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 3493547737088.0000 - mape: 43.8740 - val_loss: 3008197296128.0000 - val_mape: 42.2880\n",
      "Epoch 21/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 2864319037440.0000 - mape: 42.9731 - val_loss: 2861406617600.0000 - val_mape: 43.4724\n",
      "Epoch 22/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 2190569242624.0000 - mape: 42.6555 - val_loss: 2702848557056.0000 - val_mape: 45.1779\n",
      "Epoch 23/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 2538647584768.0000 - mape: 43.3238 - val_loss: 2552929714176.0000 - val_mape: 42.2853\n",
      "Epoch 24/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 2887389282304.0000 - mape: 42.1324 - val_loss: 2396159737856.0000 - val_mape: 40.6488\n",
      "Epoch 25/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 2152824832000.0000 - mape: 40.9270 - val_loss: 2225277763584.0000 - val_mape: 40.2789\n",
      "Epoch 26/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 2240152862720.0000 - mape: 40.5679 - val_loss: 2055937589248.0000 - val_mape: 40.4717\n",
      "Epoch 27/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 2082516107264.0000 - mape: 40.7145 - val_loss: 1884885614592.0000 - val_mape: 39.5027\n",
      "Epoch 28/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 2097446912000.0000 - mape: 40.2501 - val_loss: 1714164072448.0000 - val_mape: 38.8004\n",
      "Epoch 29/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 1352451751936.0000 - mape: 38.5461 - val_loss: 1530524336128.0000 - val_mape: 39.4233\n",
      "Epoch 30/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 1479254736896.0000 - mape: 38.8818 - val_loss: 1367170875392.0000 - val_mape: 37.1119\n",
      "Epoch 31/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 1343302664192.0000 - mape: 37.8203 - val_loss: 1202435915776.0000 - val_mape: 38.4009\n",
      "Epoch 32/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 1120999964672.0000 - mape: 37.8305 - val_loss: 1052076605440.0000 - val_mape: 35.7070\n",
      "Epoch 33/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 821905522688.0000 - mape: 35.4059 - val_loss: 903423393792.0000 - val_mape: 35.3998\n",
      "Epoch 34/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 759776935936.0000 - mape: 34.9671 - val_loss: 780393447424.0000 - val_mape: 35.5058\n",
      "Epoch 35/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 676245209088.0000 - mape: 35.2106 - val_loss: 672755744768.0000 - val_mape: 33.9161\n",
      "Epoch 36/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 670953635840.0000 - mape: 33.6482 - val_loss: 578691923968.0000 - val_mape: 33.2940\n",
      "Epoch 37/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 632054808576.0000 - mape: 32.9320 - val_loss: 512381616128.0000 - val_mape: 32.4092\n",
      "Epoch 38/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 571955544064.0000 - mape: 32.7619 - val_loss: 457784590336.0000 - val_mape: 31.5211\n",
      "Epoch 39/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 505862815744.0000 - mape: 31.6625 - val_loss: 428060442624.0000 - val_mape: 31.3979\n",
      "Epoch 40/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 471420928000.0000 - mape: 31.2987 - val_loss: 403760185344.0000 - val_mape: 31.4645\n",
      "Epoch 41/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 488798945280.0000 - mape: 31.0802 - val_loss: 392999534592.0000 - val_mape: 31.8744\n",
      "Epoch 42/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 454418333696.0000 - mape: 31.1114 - val_loss: 384010485760.0000 - val_mape: 31.3808\n",
      "Epoch 43/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 407329341440.0000 - mape: 30.2244 - val_loss: 380796665856.0000 - val_mape: 30.1232\n",
      "Epoch 44/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 436268990464.0000 - mape: 30.0675 - val_loss: 378248200192.0000 - val_mape: 29.8859\n",
      "Epoch 45/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 443877097472.0000 - mape: 30.2518 - val_loss: 376392089600.0000 - val_mape: 29.5949\n",
      "Epoch 46/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 428848807936.0000 - mape: 30.0308 - val_loss: 376069554176.0000 - val_mape: 30.1168\n",
      "Epoch 47/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 434976423936.0000 - mape: 30.3599 - val_loss: 374789210112.0000 - val_mape: 30.1016\n",
      "Epoch 48/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 422801571840.0000 - mape: 29.9310 - val_loss: 374280486912.0000 - val_mape: 29.2743\n",
      "Epoch 49/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 450531131392.0000 - mape: 29.7863 - val_loss: 371564937216.0000 - val_mape: 29.5582\n",
      "Epoch 50/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 417830010880.0000 - mape: 29.1487 - val_loss: 371502383104.0000 - val_mape: 29.9395\n",
      "Epoch 51/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 431761358848.0000 - mape: 30.1090 - val_loss: 370207195136.0000 - val_mape: 30.6207\n",
      "Epoch 52/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 443757887488.0000 - mape: 30.3004 - val_loss: 369302405120.0000 - val_mape: 29.9809\n",
      "Epoch 53/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 413742333952.0000 - mape: 29.5013 - val_loss: 369495572480.0000 - val_mape: 30.3439\n",
      "Epoch 54/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 415616499712.0000 - mape: 29.5172 - val_loss: 369518575616.0000 - val_mape: 29.0530\n",
      "Epoch 55/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 352366624768.0000 - mape: 29.2144 - val_loss: 369685495808.0000 - val_mape: 30.3158\n",
      "Epoch 56/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 448583827456.0000 - mape: 29.6059 - val_loss: 369769742336.0000 - val_mape: 30.2443\n",
      "Epoch 57/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 383173722112.0000 - mape: 29.3378 - val_loss: 367908782080.0000 - val_mape: 29.0880\n",
      "Epoch 58/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 385119125504.0000 - mape: 28.9705 - val_loss: 368230498304.0000 - val_mape: 30.7338\n",
      "Epoch 59/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 371700236288.0000 - mape: 29.5183 - val_loss: 367612657664.0000 - val_mape: 30.1742\n",
      "Epoch 60/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 383620087808.0000 - mape: 29.2665 - val_loss: 366854668288.0000 - val_mape: 29.8271\n",
      "Epoch 61/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 393248243712.0000 - mape: 29.4071 - val_loss: 367335440384.0000 - val_mape: 29.0263\n",
      "Epoch 62/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 375917019136.0000 - mape: 29.2618 - val_loss: 367481552896.0000 - val_mape: 28.9292\n",
      "Epoch 63/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 374628515840.0000 - mape: 28.9557 - val_loss: 367103148032.0000 - val_mape: 29.4387\n",
      "Epoch 64/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 376514117632.0000 - mape: 29.2594 - val_loss: 366618312704.0000 - val_mape: 29.6570\n",
      "Epoch 65/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 424764669952.0000 - mape: 29.1301 - val_loss: 366722318336.0000 - val_mape: 29.6833\n",
      "Epoch 66/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 450107310080.0000 - mape: 29.2588 - val_loss: 366556905472.0000 - val_mape: 29.0174\n",
      "Epoch 67/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 399379038208.0000 - mape: 28.8055 - val_loss: 366568734720.0000 - val_mape: 29.6491\n",
      "Epoch 68/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 409668157440.0000 - mape: 28.9676 - val_loss: 366156283904.0000 - val_mape: 29.9859\n",
      "Epoch 69/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 431916154880.0000 - mape: 29.4384 - val_loss: 368863870976.0000 - val_mape: 30.9849\n",
      "Epoch 70/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 446972133376.0000 - mape: 29.5863 - val_loss: 365945061376.0000 - val_mape: 29.7703\n",
      "Epoch 71/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 454262292480.0000 - mape: 29.4820 - val_loss: 365858455552.0000 - val_mape: 29.8252\n",
      "Epoch 72/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 394283483136.0000 - mape: 29.5634 - val_loss: 366340636672.0000 - val_mape: 29.2117\n",
      "Epoch 73/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 455694680064.0000 - mape: 29.4367 - val_loss: 366022688768.0000 - val_mape: 29.3224\n",
      "Epoch 74/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 442149109760.0000 - mape: 29.0487 - val_loss: 365559414784.0000 - val_mape: 30.2332\n",
      "Epoch 75/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 406159130624.0000 - mape: 29.2486 - val_loss: 366273953792.0000 - val_mape: 29.7874\n",
      "Epoch 76/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 418071904256.0000 - mape: 29.2441 - val_loss: 367157477376.0000 - val_mape: 30.0751\n",
      "Epoch 77/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 397092028416.0000 - mape: 29.0431 - val_loss: 367325478912.0000 - val_mape: 30.7742\n",
      "Epoch 78/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 417060585472.0000 - mape: 29.4436 - val_loss: 367721742336.0000 - val_mape: 29.8596\n",
      "Epoch 79/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 377461735424.0000 - mape: 28.9617 - val_loss: 367311486976.0000 - val_mape: 28.4143\n",
      "Epoch 80/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 429405208576.0000 - mape: 28.7091 - val_loss: 366049492992.0000 - val_mape: 28.8354\n",
      "Epoch 81/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 384503316480.0000 - mape: 28.7726 - val_loss: 366623391744.0000 - val_mape: 29.0166\n",
      "Epoch 82/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 444797124608.0000 - mape: 28.9673 - val_loss: 366354726912.0000 - val_mape: 29.3341\n",
      "Epoch 83/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 395948523520.0000 - mape: 29.0321 - val_loss: 365832208384.0000 - val_mape: 30.0342\n",
      "Epoch 84/100\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 390409289728.0000 - mape: 28.8298 - val_loss: 366838775808.0000 - val_mape: 29.2511\n"
     ]
    }
   ],
   "source": [
    "model = build_tuned_nn(X_train.shape[1])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=early_stop,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c99d18-553f-4c12-8222-74748f7d27c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
